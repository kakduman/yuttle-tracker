{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WYwyrVHN7iO"
      },
      "source": [
        "**Exercise 7: Image classification with Neural Networks**\n",
        "\n",
        "*CPSC 381/581: Machine Learning*\n",
        "\n",
        "*Yale University*\n",
        "\n",
        "*Instructor: Alex Wong*\n",
        "\n",
        "**Prerequisites**:\n",
        "\n",
        "1. Enable Google Colaboratory as an app on your Google Drive account\n",
        "\n",
        "2. Create a new Google Colab notebook, this will also create a \"Colab Notebooks\" directory under \"MyDrive\" i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks\n",
        "```\n",
        "\n",
        "3. Create the following directory structure in your Google Drive\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "\n",
        "4. Move the 07_exercise_exercise_image_classification.ipynb into\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises\n",
        "```\n",
        "so that its absolute path is\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/07_exercise_exercise_image_classification.ipynb\n",
        "```\n",
        "\n",
        "5. Prior to starting this exercise, please create a directory called 'data' within your 'Exercises' directory and within 'data' create a directory called 'exercise_07', i.e.\n",
        "```\n",
        "/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises/data/exercise_07\n",
        "```\n",
        "\n",
        "6. Set up GPU runtime by selecting `Runtime` on the top tool bar, then selecting `Change runtime type` in the drop-down menu, selecting `GPU` under Hardware accelerator and clicking `Save`.\n",
        "\n",
        "\n",
        "\n",
        "In this exercise, we will create a simple neural network model and a logistic regression model for classifying images. We will experiment with learning rate, batch size, and different configurations of layers within the network. We will demonstrate this on the CIFAR-10 dataset. Note: Accuracy of Neural Network should exceed 52%.\n",
        "\n",
        "**Submission**:\n",
        "\n",
        "1. Implement all TODOs in the code blocks below.\n",
        "\n",
        "2. Report your training and testing scores.\n",
        "\n",
        "```\n",
        "Logistic Regression:\n",
        "Epoch=1/80  Loss: 11.899\n",
        "Epoch=2/80  Loss: 9.886\n",
        "Epoch=3/80  Loss: 9.296\n",
        "Epoch=4/80  Loss: 9.306\n",
        "Epoch=5/80  Loss: 9.207\n",
        "Epoch=6/80  Loss: 8.826\n",
        "Epoch=7/80  Loss: 8.928\n",
        "Epoch=8/80  Loss: 8.770\n",
        "Epoch=9/80  Loss: 8.456\n",
        "Epoch=10/80  Loss: 8.580\n",
        "Epoch=11/80  Loss: 8.553\n",
        "Epoch=12/80  Loss: 8.578\n",
        "Epoch=13/80  Loss: 8.335\n",
        "Epoch=14/80  Loss: 8.288\n",
        "Epoch=15/80  Loss: 8.429\n",
        "Epoch=16/80  Loss: 8.243\n",
        "Epoch=17/80  Loss: 8.291\n",
        "Epoch=18/80  Loss: 8.260\n",
        "Epoch=19/80  Loss: 8.033\n",
        "Epoch=20/80  Loss: 8.376\n",
        "Epoch=21/80  Loss: 3.032\n",
        "Epoch=22/80  Loss: 3.027\n",
        "Epoch=23/80  Loss: 3.090\n",
        "Epoch=24/80  Loss: 3.148\n",
        "Epoch=25/80  Loss: 3.193\n",
        "Epoch=26/80  Loss: 3.168\n",
        "Epoch=27/80  Loss: 3.154\n",
        "Epoch=28/80  Loss: 3.202\n",
        "Epoch=29/80  Loss: 3.245\n",
        "Epoch=30/80  Loss: 3.301\n",
        "Epoch=31/80  Loss: 3.267\n",
        "Epoch=32/80  Loss: 3.227\n",
        "Epoch=33/80  Loss: 3.293\n",
        "Epoch=34/80  Loss: 3.298\n",
        "Epoch=35/80  Loss: 3.256\n",
        "Epoch=36/80  Loss: 3.275\n",
        "Epoch=37/80  Loss: 3.282\n",
        "Epoch=38/80  Loss: 3.296\n",
        "Epoch=39/80  Loss: 3.223\n",
        "Epoch=40/80  Loss: 3.327\n",
        "Epoch=41/80  Loss: 1.835\n",
        "Epoch=42/80  Loss: 1.821\n",
        "Epoch=43/80  Loss: 1.815\n",
        "Epoch=44/80  Loss: 1.812\n",
        "Epoch=45/80  Loss: 1.814\n",
        "Epoch=46/80  Loss: 1.810\n",
        "Epoch=47/80  Loss: 1.808\n",
        "Epoch=48/80  Loss: 1.819\n",
        "Epoch=49/80  Loss: 1.811\n",
        "Epoch=50/80  Loss: 1.808\n",
        "Epoch=51/80  Loss: 1.815\n",
        "Epoch=52/80  Loss: 1.810\n",
        "Epoch=53/80  Loss: 1.802\n",
        "Epoch=54/80  Loss: 1.794\n",
        "Epoch=55/80  Loss: 1.804\n",
        "Epoch=56/80  Loss: 1.796\n",
        "Epoch=57/80  Loss: 1.794\n",
        "Epoch=58/80  Loss: 1.789\n",
        "Epoch=59/80  Loss: 1.802\n",
        "Epoch=60/80  Loss: 1.790\n",
        "Epoch=61/80  Loss: 1.630\n",
        "Epoch=62/80  Loss: 1.630\n",
        "Epoch=63/80  Loss: 1.627\n",
        "Epoch=64/80  Loss: 1.628\n",
        "Epoch=65/80  Loss: 1.624\n",
        "Epoch=66/80  Loss: 1.623\n",
        "Epoch=67/80  Loss: 1.623\n",
        "Epoch=68/80  Loss: 1.625\n",
        "Epoch=69/80  Loss: 1.625\n",
        "Epoch=70/80  Loss: 1.621\n",
        "Epoch=71/80  Loss: 1.619\n",
        "Epoch=72/80  Loss: 1.621\n",
        "Epoch=73/80  Loss: 1.621\n",
        "Epoch=74/80  Loss: 1.623\n",
        "Epoch=75/80  Loss: 1.620\n",
        "Epoch=76/80  Loss: 1.621\n",
        "Epoch=77/80  Loss: 1.619\n",
        "Epoch=78/80  Loss: 1.619\n",
        "Epoch=79/80  Loss: 1.619\n",
        "Epoch=80/80  Loss: 1.618\n",
        "\n",
        "Mean accuracy over 10000 images: 38.490%\n",
        "\n",
        "==========\n",
        "\n",
        "Neural Network:\n",
        "\n",
        "Epoch=1/80  Loss: 2.078\n",
        "Epoch=2/80  Loss: 1.844\n",
        "Epoch=3/80  Loss: 1.729\n",
        "Epoch=4/80  Loss: 1.648\n",
        "Epoch=5/80  Loss: 1.587\n",
        "Epoch=6/80  Loss: 1.536\n",
        "Epoch=7/80  Loss: 1.489\n",
        "Epoch=8/80  Loss: 1.444\n",
        "Epoch=9/80  Loss: 1.407\n",
        "Epoch=10/80  Loss: 1.373\n",
        "Epoch=11/80  Loss: 1.335\n",
        "Epoch=12/80  Loss: 1.296\n",
        "Epoch=13/80  Loss: 1.257\n",
        "Epoch=14/80  Loss: 1.214\n",
        "Epoch=15/80  Loss: 1.171\n",
        "Epoch=16/80  Loss: 1.125\n",
        "Epoch=17/80  Loss: 1.078\n",
        "Epoch=18/80  Loss: 1.021\n",
        "Epoch=19/80  Loss: 0.963\n",
        "Epoch=20/80  Loss: 0.906\n",
        "Epoch=21/80  Loss: 0.564\n",
        "Epoch=22/80  Loss: 0.437\n",
        "Epoch=23/80  Loss: 0.359\n",
        "Epoch=24/80  Loss: 0.306\n",
        "Epoch=25/80  Loss: 0.264\n",
        "Epoch=26/80  Loss: 0.234\n",
        "Epoch=27/80  Loss: 0.208\n",
        "Epoch=28/80  Loss: 0.204\n",
        "Epoch=29/80  Loss: 0.152\n",
        "Epoch=30/80  Loss: 0.125\n",
        "Epoch=31/80  Loss: 0.173\n",
        "Epoch=32/80  Loss: 0.107\n",
        "Epoch=33/80  Loss: 0.125\n",
        "Epoch=34/80  Loss: 0.120\n",
        "Epoch=35/80  Loss: 0.078\n",
        "Epoch=36/80  Loss: 0.093\n",
        "Epoch=37/80  Loss: 0.094\n",
        "Epoch=38/80  Loss: 0.104\n",
        "Epoch=39/80  Loss: 0.059\n",
        "Epoch=40/80  Loss: 0.056\n",
        "Epoch=41/80  Loss: 0.012\n",
        "Epoch=42/80  Loss: 0.003\n",
        "Epoch=43/80  Loss: 0.002\n",
        "Epoch=44/80  Loss: 0.001\n",
        "Epoch=45/80  Loss: 0.001\n",
        "Epoch=46/80  Loss: 0.001\n",
        "Epoch=47/80  Loss: 0.001\n",
        "Epoch=48/80  Loss: 0.001\n",
        "Epoch=49/80  Loss: 0.001\n",
        "Epoch=50/80  Loss: 0.001\n",
        "Epoch=51/80  Loss: 0.001\n",
        "Epoch=52/80  Loss: 0.000\n",
        "Epoch=53/80  Loss: 0.000\n",
        "Epoch=54/80  Loss: 0.000\n",
        "Epoch=55/80  Loss: 0.000\n",
        "Epoch=56/80  Loss: 0.000\n",
        "Epoch=57/80  Loss: 0.000\n",
        "Epoch=58/80  Loss: 0.000\n",
        "Epoch=59/80  Loss: 0.000\n",
        "Epoch=60/80  Loss: 0.000\n",
        "Epoch=61/80  Loss: 0.000\n",
        "Epoch=62/80  Loss: 0.000\n",
        "Epoch=63/80  Loss: 0.000\n",
        "Epoch=64/80  Loss: 0.000\n",
        "Epoch=65/80  Loss: 0.000\n",
        "Epoch=66/80  Loss: 0.000\n",
        "Epoch=67/80  Loss: 0.000\n",
        "Epoch=68/80  Loss: 0.000\n",
        "Epoch=69/80  Loss: 0.000\n",
        "Epoch=70/80  Loss: 0.000\n",
        "Epoch=71/80  Loss: 0.000\n",
        "Epoch=72/80  Loss: 0.000\n",
        "Epoch=73/80  Loss: 0.000\n",
        "Epoch=74/80  Loss: 0.000\n",
        "Epoch=75/80  Loss: 0.000\n",
        "Epoch=76/80  Loss: 0.000\n",
        "Epoch=77/80  Loss: 0.000\n",
        "Epoch=78/80  Loss: 0.000\n",
        "Epoch=79/80  Loss: 0.000\n",
        "Epoch=80/80  Loss: 0.000\n",
        "\n",
        "Mean accuracy over 10000 images: 52.370%\n",
        "```\n",
        "\n",
        "3. List any collaborators.\n",
        "\n",
        "```\n",
        "Collaborators: Goolsbee, Addison \n",
        "\n",
        "Collaboration details: Discussed implementation details with Addison Goolsbee.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjvZjKr-N_qh"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "C0VFIdrB0u58",
        "outputId": "950b23ab-6ee2-4869-c7c0-b27b18be0bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/CPSC 381-581: Machine Learning/Exercises')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oGW3cccIofOE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# DEVICE = 'cpu'\n",
        "# DEVICE = 'mps'\n",
        "DEVICE = 'mps'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pft5E9Cy4w4I"
      },
      "source": [
        "Hyper-parameters for training neural network and logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uZzL5RcuTJQa"
      },
      "outputs": [],
      "source": [
        "# TODO: Choose hyper-parameters\n",
        "\n",
        "# Model - either neural network or logistic regression\n",
        "MODEL_NAME = 'neural_network'\n",
        "\n",
        "# Batch size - number of images within a training batch of one training iteration\n",
        "N_BATCH = 64\n",
        "\n",
        "# Training epoch - number of passes through the full training dataset\n",
        "N_EPOCH = 80\n",
        "\n",
        "# Learning rate - step size to update parameters\n",
        "LEARNING_RATE = 1e-1\n",
        "\n",
        "# Learning rate decay - scaling factor to decrease learning rate at the end of each decay period\n",
        "LEARNING_RATE_DECAY = 0.5\n",
        "\n",
        "# Learning rate decay period - number of epochs before reducing/decaying learning rate\n",
        "LEARNING_RATE_DECAY_PERIOD = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHOCFhqgVtO8"
      },
      "source": [
        "Define Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9YxBsFGBost_"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(torch.nn.Module):\n",
        "    '''\n",
        "    Neural network class of fully connected layers\n",
        "\n",
        "    Arg(s):\n",
        "        n_input_feature : int\n",
        "            number of input features\n",
        "        n_output : int\n",
        "            number of output classes\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_input_feature, n_output):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        # Create your 6-layer neural network using fully connected layers with ReLU activations\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
        "\n",
        "        # TODO: Instantiate 5 fully connected layers\n",
        "        # self.fully_connected_layer_1 = torch.nn.Linear(n_input_feature, 32)\n",
        "        # self.fully_connected_layer_2 = torch.nn.Linear(32, 100)\n",
        "        # self.fully_connected_layer_3 = torch.nn.Linear(100, 400)\n",
        "        # self.fully_connected_layer_4 = torch.nn.Linear(400, 1000)\n",
        "        # self.fully_connected_layer_5 = torch.nn.Linear(1000, 128)\n",
        "        self.fully_connected_layer_1 = torch.nn.Linear(n_input_feature, 50)\n",
        "        self.fully_connected_layer_2 = torch.nn.Linear(50, 500)\n",
        "        # self.fully_connected_layer_3 = torch.nn.Linear(500, 5000)\n",
        "        # self.fully_connected_layer_4 = torch.nn.Linear(5000, 500)\n",
        "        self.fully_connected_layer_5 = torch.nn.Linear(500, 128)\n",
        "\n",
        "        # TODO: Define output layer\n",
        "        self.output = torch.nn.Linear(128, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass through the neural network\n",
        "\n",
        "        Arg(s):\n",
        "            x : torch.Tensor[float32]\n",
        "                tensor of N x d\n",
        "        Returns:\n",
        "            torch.Tensor[float32]\n",
        "                tensor of n_output predicted class\n",
        "        '''\n",
        "\n",
        "        # TODO: Implement forward function\n",
        "        output_fc1 = torch.nn.functional.relu(self.fully_connected_layer_1(x)) # equivalent to writing self.fully_connected_layer_1.forward(x) because calling the object itself calls the forward function\n",
        "        output_fc2 = torch.nn.functional.relu(self.fully_connected_layer_2(output_fc1))\n",
        "        # output_fc3 = torch.nn.functional.relu(self.fully_connected_layer_3(output_fc2))\n",
        "        # output_fc4 = torch.nn.functional.relu(self.fully_connected_layer_4(output_fc3))\n",
        "        output_fc5 = torch.nn.functional.relu(self.fully_connected_layer_5(output_fc2))\n",
        "\n",
        "        output_logits = self.output(output_fc5) # we don't want to activate the logits!!!\n",
        "\n",
        "        return output_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.load_dataset import load_dataset, read_compressed_data\n",
        "\n",
        "data = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00414518376965307, 0.026042013301566348, 0.9996608492599883, 106, 0.00016, 4, {10: 0.04367, 2: 0.0455, 5: 0.04733, 52: 0.04978, 41: 0.05222, 20: 0.05283, 108: 0.05374, 106: 0.00139, 34: 0.00261, 101: 0.00504, 47: 0.00625, 100: 0.00747, 102: 0.00868, 105: 0.01019, 69: 0.01142, 139: 0.01415, 136: 0.01476, 130: 0.01568, 129: 0.0163, 140: 0.01721, 133: 0.01812, 135: 0.01995, 138: 0.02116, 97: 0.02268, 66: 0.02359, 63: 0.02572, 42: 0.02694, 98: 0.02846, 38: 0.02997, 39: 0.03301, 72: 0.03391, 43: 0.03877}], [0.00414518376965307, 0.026042013301566348, 0.9996608492599883, 106, 0.00046, 4, {10: 0.04367, 2: 0.0455, 5: 0.04733, 52: 0.04978, 41: 0.05222, 20: 0.05283, 108: 0.05374, 106: 0.00139, 34: 0.00261, 101: 0.00504, 47: 0.00625, 100: 0.00747, 102: 0.00868, 105: 0.01019, 69: 0.01142, 139: 0.01415, 136: 0.01476, 130: 0.01568, 129: 0.0163, 140: 0.01721, 133: 0.01812, 135: 0.01995, 138: 0.02116, 97: 0.02268, 66: 0.02359, 63: 0.02572, 42: 0.02694, 98: 0.02846, 38: 0.02997, 39: 0.03301, 72: 0.03391, 43: 0.03877}], [0.00414518376965307, 0.026042013301566348, 0.9996608492599883, 106, 0.00078, 4, {10: 0.04367, 2: 0.0455, 5: 0.04733, 52: 0.04978, 41: 0.05222, 20: 0.05283, 108: 0.05374, 106: 0.00139, 34: 0.00261, 101: 0.00504, 47: 0.00625, 100: 0.00747, 102: 0.00868, 105: 0.01019, 69: 0.01142, 139: 0.01415, 136: 0.01476, 130: 0.01568, 129: 0.0163, 140: 0.01721, 133: 0.01812, 135: 0.01995, 138: 0.02116, 97: 0.02268, 66: 0.02359, 63: 0.02572, 42: 0.02694, 98: 0.02846, 38: 0.02997, 39: 0.03301, 72: 0.03391, 43: 0.03877}], [0.009697375959529547, 0.06089271630234794, 0.9981443167705368, 106, 0.00108, 4, {10: 0.04367, 2: 0.0455, 5: 0.04733, 52: 0.04978, 41: 0.05222, 20: 0.05283, 108: 0.05374, 106: 0.00139, 34: 0.00261, 101: 0.00504, 47: 0.00625, 100: 0.00747, 102: 0.00868, 105: 0.01019, 69: 0.01142, 139: 0.01415, 136: 0.01476, 130: 0.01568, 129: 0.0163, 140: 0.01721, 133: 0.01812, 135: 0.01995, 138: 0.02116, 97: 0.02268, 66: 0.02359, 63: 0.02572, 42: 0.02694, 98: 0.02846, 38: 0.02997, 39: 0.03301, 72: 0.03391, 43: 0.03877}], [0.01601862272387442, 0.10047813340730377, 0.9949392668434511, 106, 0.00139, 4, {10: 0.04367, 2: 0.0455, 5: 0.04733, 52: 0.04978, 41: 0.05222, 20: 0.05283, 108: 0.05374, 106: 0.00139, 34: 0.00261, 101: 0.00504, 47: 0.00625, 100: 0.00747, 102: 0.00868, 105: 0.01019, 69: 0.01142, 139: 0.01415, 136: 0.01476, 130: 0.01568, 129: 0.0163, 140: 0.01721, 133: 0.01812, 135: 0.01995, 138: 0.02116, 97: 0.02268, 66: 0.02359, 63: 0.02572, 42: 0.02694, 98: 0.02846, 38: 0.02997, 39: 0.03301, 72: 0.03391, 43: 0.03877}]]\n"
          ]
        }
      ],
      "source": [
        "print(data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.inputs = [torch.tensor(d[:5], dtype=torch.float32) for d in data]\n",
        "        self.targets = [torch.tensor(list(d[6].values()), dtype=torch.float32) for d in data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "dataset = MyDataset(data)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdnyLTLbZxTb"
      },
      "source": [
        "Define training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zTEHZSxEZxsE"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          dataloader,\n",
        "          n_epoch,\n",
        "          optimizer,\n",
        "          learning_rate_decay,\n",
        "          learning_rate_decay_period,\n",
        "          device):\n",
        "    '''\n",
        "    Trains the model using optimizer and specified learning rate schedule\n",
        "\n",
        "    Arg(s):\n",
        "        model : torch.nn.Module\n",
        "            neural network or logistic regression\n",
        "        dataloader : torch.utils.data.DataLoader\n",
        "            # https://pytorch.org/docs/stable/data.html\n",
        "            dataloader for training data\n",
        "        n_epoch : int\n",
        "            number of epochs to train\n",
        "        optimizer : torch.optim\n",
        "            https://pytorch.org/docs/stable/optim.html\n",
        "            optimizer to use for updating weights\n",
        "        learning_rate_decay : float\n",
        "            rate of learning rate decay\n",
        "        learning_rate_decay_period : int\n",
        "            period to reduce learning rate based on decay e.g. every 2 epoch\n",
        "        device : str\n",
        "            device to run on\n",
        "    Returns:\n",
        "        torch.nn.Module : trained network\n",
        "    '''\n",
        "\n",
        "    device = 'cuda' if device == 'gpu' or device == 'cuda' else 'cpu'\n",
        "    device = DEVICE # since running this locally on mac\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # loss function: mean absolute error\n",
        "    loss_func = torch.nn.L1Loss()\n",
        "   \n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss = 0\n",
        "        if epoch and epoch % learning_rate_decay_period == 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= learning_rate_decay\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward through the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Clear gradients so we don't accumulate them from previous batches\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute loss function\n",
        "            loss = loss_func(outputs, targets)\n",
        "\n",
        "            # Update parameters by backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate total loss for the epoch\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        mean_loss = total_loss / len(dataloader)\n",
        "        print('Epoch={}/{}  Loss: {:.3f}'.format(epoch + 1, n_epoch, mean_loss))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XKF-hNYa7ea"
      },
      "source": [
        "Define evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pa7O3vsuZ9S7"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    '''\n",
        "    Evaluates the network on a dataset\n",
        "\n",
        "    Arg(s):\n",
        "        model : torch.nn.Module\n",
        "            neural network\n",
        "        dataloader : torch.utils.data.DataLoader\n",
        "            dataloader for testing data\n",
        "        device : str\n",
        "            device to run on\n",
        "    '''\n",
        "    device = torch.device(DEVICE)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "    loss_func = torch.nn.L1Loss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward through the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss function\n",
        "            loss = loss_func(outputs, targets)\n",
        "            total_loss += loss.item() * targets.size(0)\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    mean_loss = total_loss / total_samples\n",
        "    print('Mean absolute error: {:.3f}'.format(mean_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80 - Loss: 0.3827\n",
            "Model saved to ./model_checkpoint.pth after epoch 1\n",
            "Epoch 2/80 - Loss: 0.2458\n",
            "Model saved to ./model_checkpoint.pth after epoch 2\n",
            "Epoch 3/80 - Loss: 0.2462\n",
            "Model saved to ./model_checkpoint.pth after epoch 3\n",
            "Epoch 4/80 - Loss: 0.2457\n",
            "Model saved to ./model_checkpoint.pth after epoch 4\n",
            "Epoch 5/80 - Loss: 0.2463\n",
            "Model saved to ./model_checkpoint.pth after epoch 5\n",
            "Epoch 6/80 - Loss: 0.2459\n",
            "Model saved to ./model_checkpoint.pth after epoch 6\n",
            "Epoch 7/80 - Loss: 0.2463\n",
            "Model saved to ./model_checkpoint.pth after epoch 7\n",
            "Epoch 8/80 - Loss: 0.2454\n",
            "Model saved to ./model_checkpoint.pth after epoch 8\n",
            "Epoch 9/80 - Loss: 0.2461\n",
            "Model saved to ./model_checkpoint.pth after epoch 9\n",
            "Epoch 10/80 - Loss: 0.2463\n",
            "Model saved to ./model_checkpoint.pth after epoch 10\n",
            "Epoch 11/80 - Loss: 0.2456\n",
            "Model saved to ./model_checkpoint.pth after epoch 11\n",
            "Epoch 12/80 - Loss: 0.2460\n",
            "Model saved to ./model_checkpoint.pth after epoch 12\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Train and save the model\u001b[39;00m\n\u001b[1;32m     73\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtrain_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE_DECAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE_DECAY_PERIOD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[21], line 51\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[0;34m(model, train_loader, n_epoch, learning_rate, learning_rate_decay, learning_rate_decay_period, device, checkpoint_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, targets)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/Documents/GitHub/yuttle-tracker/.venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/yuttle-tracker/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/GitHub/yuttle-tracker/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_and_save(model, train_loader, n_epoch, learning_rate, learning_rate_decay, learning_rate_decay_period, device, checkpoint_path):\n",
        "    '''\n",
        "    Trains the model using optimizer and specified learning rate schedule, and saves the model checkpoint.\n",
        "\n",
        "    Arg(s):\n",
        "        model : torch.nn.Module\n",
        "            Neural network model\n",
        "        train_loader : torch.utils.data.DataLoader\n",
        "            DataLoader for training data\n",
        "        n_epoch : int\n",
        "            Number of epochs to train\n",
        "        learning_rate : float\n",
        "            Initial learning rate\n",
        "        learning_rate_decay : float\n",
        "            Rate of learning rate decay\n",
        "        learning_rate_decay_period : int\n",
        "            Period to reduce learning rate based on decay e.g. every 2 epochs\n",
        "        device : str\n",
        "            Device to run on (e.g., 'cuda', 'mps', 'cpu')\n",
        "        checkpoint_path : str\n",
        "            Path to save the model checkpoint\n",
        "    '''\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_func = torch.nn.L1Loss()\n",
        "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss = 0\n",
        "        if epoch % learning_rate_decay_period == 0 and epoch > 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= learning_rate_decay\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_func(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        print(f'Epoch {epoch+1}/{n_epoch} - Loss: {total_loss / len(train_loader):.4f}')\n",
        "\n",
        "        # Save the model checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': total_loss / len(train_loader)\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        print(f'Model saved to {checkpoint_path} after epoch {epoch+1}')\n",
        "\n",
        "# Define model\n",
        "model = NeuralNetwork(5, 32)  # Adjust these parameters based on your model's input and output dimensions\n",
        "\n",
        "# Train and save the model\n",
        "checkpoint_path = './model_checkpoint.pth'\n",
        "train_and_save(model, train_loader, N_EPOCH, LEARNING_RATE, LEARNING_RATE_DECAY, LEARNING_RATE_DECAY_PERIOD, DEVICE, checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMVEF-Ajn691"
      },
      "source": [
        "Training a neural network for image classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LxKRWZZQn5VY"
      },
      "outputs": [],
      "source": [
        "# Create transformations convert data to torch tensor\n",
        "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Set path to save checkpoint\n",
        "checkpoint_path = './checkpoint-{}.pth'.format(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8byUq18XoAN-",
        "outputId": "93b93592-da27-4f7f-c0ed-94116a907185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch=1/80  Loss: 2.078\n",
            "Epoch=2/80  Loss: 1.844\n",
            "Epoch=3/80  Loss: 1.729\n",
            "Epoch=4/80  Loss: 1.648\n",
            "Epoch=5/80  Loss: 1.587\n",
            "Epoch=6/80  Loss: 1.536\n",
            "Epoch=7/80  Loss: 1.489\n",
            "Epoch=8/80  Loss: 1.444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>self._shutdown_workers()\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()    \n",
            "if w.is_alive():  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "    if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process': \n",
            "can only test a child processAssertionError\n",
            ": can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch=9/80  Loss: 1.407\n",
            "Epoch=10/80  Loss: 1.373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch=11/80  Loss: 1.335\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch=12/80  Loss: 1.296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch=13/80  Loss: 1.257\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch=14/80  Loss: 1.214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0><function _MultiProcessingDataLoaderIter.__del__ at 0x7b142084b5b0>\n",
            "Traceback (most recent call last):\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        self._shutdown_workers()\n",
            "self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        if w.is_alive():if w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
            ": AssertionErrorcan only test a child process\n",
            ": can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch=15/80  Loss: 1.171\n",
            "Epoch=16/80  Loss: 1.125\n",
            "Epoch=17/80  Loss: 1.078\n",
            "Epoch=18/80  Loss: 1.021\n",
            "Epoch=19/80  Loss: 0.963\n",
            "Epoch=20/80  Loss: 0.906\n",
            "Epoch=21/80  Loss: 0.564\n",
            "Epoch=22/80  Loss: 0.437\n",
            "Epoch=23/80  Loss: 0.359\n",
            "Epoch=24/80  Loss: 0.306\n",
            "Epoch=25/80  Loss: 0.264\n",
            "Epoch=26/80  Loss: 0.234\n",
            "Epoch=27/80  Loss: 0.208\n",
            "Epoch=28/80  Loss: 0.204\n",
            "Epoch=29/80  Loss: 0.152\n",
            "Epoch=30/80  Loss: 0.125\n",
            "Epoch=31/80  Loss: 0.173\n",
            "Epoch=32/80  Loss: 0.107\n",
            "Epoch=33/80  Loss: 0.125\n",
            "Epoch=34/80  Loss: 0.120\n",
            "Epoch=35/80  Loss: 0.078\n",
            "Epoch=36/80  Loss: 0.093\n",
            "Epoch=37/80  Loss: 0.094\n",
            "Epoch=38/80  Loss: 0.104\n",
            "Epoch=39/80  Loss: 0.059\n",
            "Epoch=40/80  Loss: 0.056\n",
            "Epoch=41/80  Loss: 0.012\n",
            "Epoch=42/80  Loss: 0.003\n",
            "Epoch=43/80  Loss: 0.002\n",
            "Epoch=44/80  Loss: 0.001\n",
            "Epoch=45/80  Loss: 0.001\n",
            "Epoch=46/80  Loss: 0.001\n",
            "Epoch=47/80  Loss: 0.001\n",
            "Epoch=48/80  Loss: 0.001\n",
            "Epoch=49/80  Loss: 0.001\n",
            "Epoch=50/80  Loss: 0.001\n",
            "Epoch=51/80  Loss: 0.001\n",
            "Epoch=52/80  Loss: 0.000\n",
            "Epoch=53/80  Loss: 0.000\n",
            "Epoch=54/80  Loss: 0.000\n",
            "Epoch=55/80  Loss: 0.000\n",
            "Epoch=56/80  Loss: 0.000\n",
            "Epoch=57/80  Loss: 0.000\n",
            "Epoch=58/80  Loss: 0.000\n",
            "Epoch=59/80  Loss: 0.000\n",
            "Epoch=60/80  Loss: 0.000\n",
            "Epoch=61/80  Loss: 0.000\n",
            "Epoch=62/80  Loss: 0.000\n",
            "Epoch=63/80  Loss: 0.000\n",
            "Epoch=64/80  Loss: 0.000\n",
            "Epoch=65/80  Loss: 0.000\n",
            "Epoch=66/80  Loss: 0.000\n",
            "Epoch=67/80  Loss: 0.000\n",
            "Epoch=68/80  Loss: 0.000\n",
            "Epoch=69/80  Loss: 0.000\n",
            "Epoch=70/80  Loss: 0.000\n",
            "Epoch=71/80  Loss: 0.000\n",
            "Epoch=72/80  Loss: 0.000\n",
            "Epoch=73/80  Loss: 0.000\n",
            "Epoch=74/80  Loss: 0.000\n",
            "Epoch=75/80  Loss: 0.000\n",
            "Epoch=76/80  Loss: 0.000\n",
            "Epoch=77/80  Loss: 0.000\n",
            "Epoch=78/80  Loss: 0.000\n",
            "Epoch=79/80  Loss: 0.000\n",
            "Epoch=80/80  Loss: 0.000\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Set up dataloading\n",
        "'''\n",
        "# Download and setup CIFAR10 training set using preconfigured torchvision.datasets.CIFAR10\n",
        "cifar10_train = torchvision.datasets.CIFAR10(\n",
        "    root=os.path.join('data', 'exercise_07'),\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms)\n",
        "\n",
        "# Setup a dataloader (iterator) to fetch from the training set using\n",
        "# torch.utils.data.DataLoader and set shuffle=True, drop_last=True, num_workers=2\n",
        "# Set your batch size to the hyperparameter N_BATCH\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    cifar10_train,\n",
        "    batch_size=N_BATCH,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2)\n",
        "\n",
        "# Define the possible classes in CIFAR10\n",
        "class_names = [\n",
        "    'plane',\n",
        "    'car',\n",
        "    'bird',\n",
        "    'cat',\n",
        "    'deer',\n",
        "    'dog',\n",
        "    'frog',\n",
        "    'horse',\n",
        "    'ship',\n",
        "    'truck'\n",
        "]\n",
        "\n",
        "# CIFAR10 has 10 classes\n",
        "n_class = len(class_names)\n",
        "\n",
        "'''\n",
        "Set up model and optimizer\n",
        "'''\n",
        "# Compute number of input features: 3 (channel) by 32 (height) by 32 (width)\n",
        "n_input_feature = 3 * 32 * 32\n",
        "\n",
        "if MODEL_NAME == 'neural_network':\n",
        "    # Instantiate neural network\n",
        "    model = NeuralNetwork(n_input_feature, n_class)\n",
        "elif MODEL_NAME == 'logistic_regression':\n",
        "    # Instantiate logistic regression\n",
        "    model = LogisticRegression(n_input_feature, n_class)\n",
        "else:\n",
        "    raise('Unsupported model name: {}'.format(MODEL_NAME))\n",
        "\n",
        "# Setup learning rate SGD optimizer and step function scheduler\n",
        "# https://pytorch.org/docs/stable/optim.html?#torch.optim.SGD\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "'''\n",
        "Train model and store weights\n",
        "'''\n",
        "# Set model to training mode\n",
        "model.train()\n",
        "\n",
        "# Train model with device='cuda'\n",
        "# I'm training locally so using mps\n",
        "model = train(model,\n",
        "              dataloader_train,\n",
        "              N_EPOCH,\n",
        "              optimizer,\n",
        "              learning_rate_decay=LEARNING_RATE_DECAY,\n",
        "              learning_rate_decay_period=LEARNING_RATE_DECAY_PERIOD,\n",
        "              device=DEVICE)\n",
        "\n",
        "# Save weights into checkpoint path\n",
        "torch.save({'state_dict': model.state_dict()}, checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WubtddGkwiSo"
      },
      "source": [
        "Testing the trained neural network on image classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KByIHECHA3m6",
        "outputId": "7f460e65-17ca-403c-db98-2dba00463dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Mean accuracy over 10000 images: 52.370%\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Set up dataloading\n",
        "'''\n",
        "# Download and setup CIFAR10 testing set using\n",
        "# preconfigured torchvision.datasets.CIFAR10\n",
        "cifar10_test = torchvision.datasets.CIFAR10(\n",
        "    root=os.path.join('data', 'exercise_07'),\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms)\n",
        "\n",
        "# Setup a dataloader (iterator) to fetch from the testing set using\n",
        "# torch.utils.data.DataLoader and set shuffle=False, drop_last=False, num_workers=2\n",
        "# Set batch_size to 25\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    cifar10_test,\n",
        "    batch_size=25,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=2)\n",
        "\n",
        "'''\n",
        "Set up model\n",
        "'''\n",
        "# Compute number of input features: 3 (channel) by 32 (height) by 32 (width)\n",
        "n_input_feature = 3 * 32 * 32\n",
        "\n",
        "if MODEL_NAME == 'neural_network':\n",
        "    # Instantiate neural network\n",
        "    model = NeuralNetwork(n_input_feature, n_class)\n",
        "elif MODEL_NAME == 'logistic_regression':\n",
        "    # Instantiate logistic regression\n",
        "    model = LogisticRegression(n_input_feature, n_class)\n",
        "else:\n",
        "    raise('Unsupported model name: {}'.format(MODEL_NAME))\n",
        "\n",
        "'''\n",
        "Restore weights and evaluate model\n",
        "'''\n",
        "# Load model from checkpoint\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "\n",
        "# Set model to evaluation mode:\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# Evaluate model on testing set with device=DEVICE\n",
        "\n",
        "evaluate(model, dataloader_test, class_names, DEVICE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnEKW0z0yG93"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
